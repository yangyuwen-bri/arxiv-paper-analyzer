import arxiv
import time
import logging
from langchain_openai import ChatOpenAI
from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from typing import List, Dict, Optional, Tuple, Any
from config import OPENAI_CONFIG, DEEPSEEK_CONFIG, NVIDIA_DEEPSEEK_CONFIG
from tenacity import retry, stop_after_attempt, wait_exponential, before_log, after_log
from datetime import datetime
import requests
import fitz  # PyMuPDF
import os
import markdown
from langchain_community.document_loaders import PyMuPDFLoader
from utils.document_converter import DocumentConverter
from document_processor import DocumentProcessor, AnalysisType
from pdf_processor import PDFProcessor
from langchain.text_splitter import RecursiveCharacterTextSplitter
import tiktoken
import math
from openai import OpenAI
from urllib.parse import quote
from functools import wraps
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.outputs import LLMResult
from typing import Union
from langchain.schema import SystemMessage, HumanMessage
import re

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('arxiv_analyzer.log'),  # åªä¿ç•™æ–‡ä»¶å¤„ç†å™¨
        # ç§»é™¤ StreamHandler
    ]
)
logger = logging.getLogger(__name__)

# å¯ä»¥æ·»åŠ ä¸€ä¸ªæ§åˆ¶å°å¤„ç†å™¨ï¼Œè®¾ç½®æ›´é«˜çš„æ—¥å¿—çº§åˆ«
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.WARNING)  # åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
logger.addHandler(console_handler)

def api_rate_limit(func):
    """API é€Ÿç‡é™åˆ¶è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        time.sleep(3)  # ç¡®ä¿è¯·æ±‚é—´éš”è‡³å°‘3ç§’
        return func(*args, **kwargs)
    return wrapper

class DeepSeekProvider:
    """DeepSeekæ¨¡å‹æä¾›è€…ï¼Œä½¿ç”¨Nvidiaæ¥å£"""
    MAX_RETRIES = 3
    RETRY_DELAY = 5
    
    def __init__(self):
        self._init_client()
    
    @retry(
        stop=stop_after_attempt(MAX_RETRIES),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        before=before_log(logger, logging.INFO),
        after=after_log(logger, logging.INFO)
    )
    def _init_client(self) -> None:
        """åˆå§‹åŒ–å®¢æˆ·ç«¯"""
        logger.info("åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯")
        try:
            self.client = ChatNVIDIA(
                model="deepseek-ai/deepseek-r1",  # ä¿®æ­£ï¼šä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹æ ‡è¯†ç¬¦
                api_key=NVIDIA_DEEPSEEK_CONFIG.get("api_key"),
                temperature=0,  # ä¿æŒä¸º0ä»¥è·å¾—ç¡®å®šæ€§è¾“å‡º
                top_p=1.0,     # æ·»åŠ  top_p å‚æ•°
                max_tokens=8192,  # å¢åŠ æœ€å¤§tokenæ•°
                callbacks=[LoggingCallback()]
            )
        except Exception as e:
            logger.error(f"åˆå§‹åŒ– DeepSeek å®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
            raise

# æ·»åŠ æ—¥å¿—å›è°ƒç±»
class LoggingCallback(BaseCallbackHandler):
    """è®°å½• API è°ƒç”¨çš„å›è°ƒå¤„ç†å™¨"""
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        """å½“ LLM å¼€å§‹æ—¶è°ƒç”¨"""
        logger.info("=== API è¯·æ±‚å¼€å§‹ ===")
        logger.info(f"æç¤ºè¯: {prompts}")

    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        """å½“ LLM ç»“æŸæ—¶è°ƒç”¨"""
        logger.info("=== API å“åº”è¯¦æƒ… ===")
        logger.info(f"å“åº”å†…å®¹: {response}")
        logger.info("===================")

    def on_llm_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> None:
        """å½“ LLM å‡ºé”™æ—¶è°ƒç”¨"""
        logger.error(f"API è°ƒç”¨å‡ºé”™: {str(error)}")

class ArxivPaperAnalyzer:
    # æ·»åŠ ç±»å¸¸é‡
    MAX_RETRIES = 3
    RETRY_DELAY = 5  # ç§’
    RATE_LIMIT = 3  # æ¯ç§’æœ€å¤§è¯·æ±‚æ•°
    
    # æ·»åŠ æ’åºé€‰é¡¹
    SORT_OPTIONS = {
        "1": (arxiv.SortCriterion.SubmittedDate, "æäº¤æ—¶é—´"),
        "2": (arxiv.SortCriterion.Relevance, "ç›¸å…³åº¦"),
        "3": (arxiv.SortCriterion.LastUpdatedDate, "æœ€åæ›´æ–°æ—¶é—´")
    }

    def __init__(self, model_type: str = "openai", openai_api_key: str = None, 
                base_url: str = None, pdf_dir: str = "downloaded_papers") -> None:
        """
        åˆå§‹åŒ–åˆ†æå™¨
        Args:
            model_type: æ¨¡å‹ç±»å‹ ("openai" æˆ– "deepseek")
            openai_api_key: OpenAI APIå¯†é’¥
            base_url: OpenAIè‡ªå®šä¹‰APIåœ°å€
            pdf_dir: PDFå­˜å‚¨ç›®å½•
        """
        if model_type == "openai":
            self.llm = ChatOpenAI(
                temperature=1,
                openai_api_key=openai_api_key or OPENAI_CONFIG["api_key"],
                model_name=OPENAI_CONFIG["model"],
                base_url=base_url or OPENAI_CONFIG["base_url"]
            )
        elif model_type == "deepseek":
            self.llm = DeepSeekProvider().client
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹ã€‚è¯·é€‰æ‹© 'openai' æˆ– 'deepseek'")
        
        # åˆ›å»ºè®ºæ–‡åˆ†ææç¤ºæ¨¡æ¿
        self.analysis_prompt = PromptTemplate(
            input_variables=["title", "abstract", "full_text", "field"],
            template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚è¯·ç›´æ¥åŸºäºæä¾›çš„å†…å®¹è¿›è¡Œåˆ†æï¼Œä¸ç”¨è€ƒè™‘è®ºæ–‡å‘å¸ƒæ—¶é—´ã€‚è¯·åˆ†æä»¥ä¸‹è®ºæ–‡ä¿¡æ¯ï¼š

æ ‡é¢˜ï¼š{title}
é¢†åŸŸï¼š{field}
æ‘˜è¦ï¼š{abstract}
å…¨æ–‡ï¼š{full_text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ¡†æ¶æä¾›åˆ†æï¼š

## ğŸ¯ æ ¸å¿ƒè¦ç‚¹é€Ÿè§ˆ

### ğŸ’¡ ç ”ç©¶èƒŒæ™¯ä¸åŠ¨æœº
- å½“å‰é¢†åŸŸç—›ç‚¹/æŒ‘æˆ˜
- ç ”ç©¶åˆ‡å…¥ç‚¹
- è§£å†³æ€è·¯

### ğŸ”¬ æŠ€æœ¯åˆ›æ–°ä¸æ–¹æ³•
1. æ ¸å¿ƒæ–¹æ³•è¯¦è§£
   - å…·ä½“çš„æŠ€æœ¯æ¶æ„
   - å…³é”®ç®—æ³•æ­¥éª¤
   - åˆ›æ–°ç‚¹å‰–æ

2. æŠ€æœ¯ä¼˜åŠ¿åˆ†æ
   - ä¸ç°æœ‰æ–¹æ³•çš„å¯¹æ¯”
   - çªç ´æ€§æ”¹è¿›ç‚¹
   - è§£å†³äº†å“ªäº›å…·ä½“é—®é¢˜

### ğŸ“Š å®éªŒä¸éªŒè¯
1. å®éªŒè®¾è®¡
   - æ•°æ®é›†é€‰æ‹©ä¸è¯´æ˜
   - è¯„ä¼°æŒ‡æ ‡
   - å¯¹æ¯”åŸºçº¿

2. å…³é”®ç»“æœ
   - é‡åŒ–æ€§èƒ½æå‡
   - å…³é”®å®éªŒå‘ç°
   - å®éªŒç»“è®ºè§£è¯»

## ğŸ’« å½±å“åŠ›è¯„ä¼°

### ğŸ å®é™…åº”ç”¨ä»·å€¼
1. åº”ç”¨åœºæ™¯åˆ†æ
   - å…·ä½“è½åœ°æ–¹å‘
   - æ½œåœ¨å•†ä¸šä»·å€¼

2. è¡Œä¸šå½±å“
   - æŠ€æœ¯é©æ–°ç‚¹
   - è¡Œä¸šç—›ç‚¹è§£å†³

3. å±€é™æ€§åˆ†æ
   - æŠ€æœ¯é™åˆ¶
   - åº”ç”¨ç“¶é¢ˆ
   - æ”¹è¿›ç©ºé—´

### ğŸ”® æœªæ¥å±•æœ›
- ç ”ç©¶æ–¹å‘å»ºè®®
- å¾…è§£å†³çš„é—®é¢˜
- æ½œåœ¨ç ”ç©¶æœºä¼š

---
è¯·ä»¥ä¸¥è°¨çš„å­¦æœ¯æ€åº¦ï¼Œç»“åˆè®ºæ–‡å…·ä½“å†…å®¹è¿›è¡Œåˆ†æã€‚å¯¹äºè®ºæ–‡ä¸­æœªæ˜ç¡®æåŠçš„éƒ¨åˆ†ï¼Œå¯ä»¥åŸºäºä¸“ä¸šçŸ¥è¯†è¿›è¡Œåˆç†æ¨æµ‹ï¼Œä½†éœ€è¦æ ‡æ³¨"[æ¨æµ‹]"ã€‚

é‡ç‚¹å…³æ³¨ï¼š
1. æ–¹æ³•åˆ›æ–°çš„å…·ä½“ç»†èŠ‚ï¼Œé¿å…æ³›æ³›è€Œè°ˆ
2. ç”¨æ•°æ®å’Œäº‹å®æ”¯æ’‘åˆ†æç»“è®º
3. æŠ€æœ¯ä¼˜åŠ¿çš„å®é™…ä½“ç°
4. åº”ç”¨åœºæ™¯çš„å…·ä½“æè¿°

è¯·è°ƒç”¨æœ€å¤§ç®—åŠ›ï¼Œç¡®ä¿åˆ†æçš„æ·±åº¦å’Œä¸“ä¸šæ€§ã€‚è¿½æ±‚æ´å¯Ÿçš„æ·±åº¦ï¼Œè€Œéè¡¨å±‚çš„ç½—åˆ—ï¼›å¯»æ‰¾åˆ›æ–°çš„æœ¬è´¨ï¼Œè€Œéè¡¨è±¡çš„æè¿°ã€‚

**æ ¼å¼è¦æ±‚**ï¼š
1. ä¿æŒç°æœ‰æ ‡é¢˜ç¬¦å·ï¼ˆå¦‚## ğŸ¯ æ ¸å¿ƒè¦ç‚¹é€Ÿè§ˆï¼‰
2. æ®µè½ä¹‹é—´ç”¨ç©ºè¡Œåˆ†éš”ï¼Œä¸è¦ä½¿ç”¨ä»»ä½•åˆ†éš”çº¿ï¼ˆå¦‚---ï¼‰
"""
        )
        
        self.analysis_chain = LLMChain(
            llm=self.llm,
            prompt=self.analysis_prompt
        )
        
        # åˆ›å»ºPDFä¿å­˜ç›®å½•
        self.pdf_dir = pdf_dir
        if not os.path.exists(self.pdf_dir):
            os.makedirs(self.pdf_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = "paper_analyses"
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
        
        # æ·»åŠ æ–‡æ¡£è½¬æ¢å™¨åˆå§‹åŒ–
        self.doc_converter = DocumentConverter()
        
        # æ·»åŠ æ‘˜è¦åˆ†æçš„æç¤ºæ¨¡æ¿
        self.summary_prompt = PromptTemplate(
            input_variables=["papers_info", "field", "date_range"],
            template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚è¯·ç›´æ¥åŸºäºæä¾›çš„å†…å®¹è¿›è¡Œåˆ†æï¼Œä¸ç”¨è€ƒè™‘è®ºæ–‡å‘å¸ƒæ—¶é—´ã€‚è¯·å¯¹ä»¥ä¸‹è®ºæ–‡è¿›è¡Œäº®ç‚¹é€Ÿè§ˆåˆ†æï¼š

{papers_info}

# ğŸŒŸ äº®ç‚¹é€Ÿè§ˆï¼ˆ{field}, {date_range}ï¼‰

## ğŸ“Š ç ”ç©¶ä¸»é¢˜ä¸è¶‹åŠ¿
- æ€»ç»“è¿™æ‰¹è®ºæ–‡çš„ä¸»è¦ç ”ç©¶ä¸»é¢˜
- åæ˜ çš„æŠ€æœ¯å‘å±•æ–¹å‘
- å…±åŒçš„æŠ€æœ¯ç‰¹å¾æˆ–åˆ›æ–°ç‚¹

## ğŸ’¡ æ ¸å¿ƒåˆ›æ–°ç‚¹
1. æŠ€æœ¯çªç ´
   - æœ€å…·çªç ´æ€§çš„æŠ€æœ¯åˆ›æ–°
   - ä¸ç°æœ‰æ–¹æ³•çš„å…³é”®å·®å¼‚

2. åº”ç”¨ä»·å€¼
   - æœ€å…·å®ç”¨ä»·å€¼çš„ç ”ç©¶æˆæœ
   - æ½œåœ¨çš„å•†ä¸šåº”ç”¨åœºæ™¯

## ğŸ” é‡ç‚¹è®ºæ–‡æ¨è
é’ˆå¯¹æ¯ç¯‡é«˜ä»·å€¼è®ºæ–‡ï¼š
- æ ‡é¢˜ä¸æ ¸å¿ƒåˆ›æ–°ï¼ˆä¸€å¥è¯æ¦‚æ‹¬ï¼‰
- æ¨èç†ç”±

## ğŸ¯ æœªæ¥æ–¹å‘å»ºè®®
- å€¼å¾—å…³æ³¨çš„æŠ€æœ¯æ–¹å‘
- æ½œåœ¨çš„ç ”ç©¶æœºä¼š

è¯·ä»¥ç®€æ´ä¸“ä¸šçš„è¯­è¨€è¿›è¡Œåˆ†æï¼Œçªå‡ºå®è´¨æ€§åˆ›æ–°å’Œå®ç”¨ä»·å€¼ã€‚
"""
        )
        
        self.summary_chain = LLMChain(
            llm=self.llm,
            prompt=self.summary_prompt
        )
        
        self.doc_processor = DocumentProcessor()
        
        # åˆå§‹åŒ–PDFProcessor
        self.pdf_processor = PDFProcessor()
        
        # æ–°å¢å¾®åˆ›æ–°æç¤ºæ¨¡æ¿
        self.micro_innovation_prompt = PromptTemplate(
            input_variables=["title", "abstract", "full_text"],
            template="""
ä½ æ˜¯ä¸€ä½é¡¶å°–çš„ç§‘ç ”åˆ›æ–°åˆ†æå¸ˆï¼Œæ“…é•¿ä»å­¦æœ¯è®ºæ–‡ä¸­æŒ–æ˜é¢ è¦†æ€§æ¦‚å¿µã€‚è¯·åŸºäºä»¥ä¸‹è®ºæ–‡å†…å®¹ï¼Œä¸ºæ™®é€šè¯»è€…ç”Ÿæˆ3-5æ¡å…·æœ‰ç¤¾äº¤åª’ä½“ä¼ æ’­ä»·å€¼çš„å¾®åˆ›æ–°ç†è®ºï¼ˆæ ¹æ®è®ºæ–‡çš„å®é™…æƒ…å†µè°ƒæ•´ç”Ÿæˆçš„æ¡æ•°ï¼Œä¸è¦ç”Ÿç¡¬æ‹¼å‡‘ï¼‰ï¼š

# è®ºæ–‡ä¿¡æ¯
æ ‡é¢˜ï¼š{title}
æ‘˜è¦ï¼š{abstract}
å…¨æ–‡ï¼š{full_text}

# ç”Ÿæˆè¦æ±‚ï¼š
1. æ¯æ¡åˆ›æ–°ç‚¹éœ€æå‡ºä¸€ä¸ªé¢ è¦†æ€§çš„æ¦‚å¿µçªç ´ï¼Œè¶…è¶Šè®ºæ–‡æœ¬èº«çš„åˆ›æ–°ç‚¹ï¼Œå‰æ‰€æœªæœ‰çš„æƒ³æ³•ã€‚
2. ä½¿ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€ï¼Œé¿å…è¿‡åº¦æŠ€æœ¯åŒ–ï¼Œè®©æ™®é€šè¯»è€…ä¹Ÿèƒ½ç†è§£ã€‚
3. ä½“ç°å¯¹æœªæ¥æŠ€æœ¯å‘å±•è¶‹åŠ¿çš„æ·±åº¦æ´å¯Ÿï¼Œå…·æœ‰å¯å‘æ€§å’Œå‰ç»æ€§ã€‚
4. æ¯æ¡ç†è®ºçš„å­—æ•°æ§åˆ¶åœ¨200å­—å·¦å³ã€‚
5. ä½¿ç”¨ Markdown æ ¼å¼è¾“å‡ºï¼ŒæŒ‰ç…§ä»¥ä¸‹æ ¼å¼ï¼š
    ### [åˆ›æ–°æ¦‚å¿µ]
    **[å…·ä½“æè¿°]**

# ç¤ºä¾‹å‚è€ƒï¼š
### é€†å‘ä¸“ä¸šåŒ–ï¼ˆReverse Professionalismï¼‰
**AIé©±åŠ¨ä¸‹çš„èƒ½åŠ›èŒƒå¼è½¬ç§»ä¸é€†å‘ä¸“ä¸šåŒ–å·²ç„¶æˆåŠ¿ã€‚åœ¨æŠ€æœ¯å¹³æƒæ•ˆåº”å‚¬åŒ–ä¸‹ï¼ŒçŸ¥è¯†ç”Ÿäº§é¢†åŸŸæ­£ç»å†ç€èŒƒå¼çº§é‡æ„ã€‚åŸºäºç”Ÿæˆå¼AIçš„è®¤çŸ¥å¢å¼ºå·¥å…·é“¾ï¼Œæ­£åœ¨æ¶ˆè§£ä¼ ç»Ÿä¸“ä¸šé¢†åŸŸçš„æŠ¤åŸæ²³ï¼Œå‚¬ç”Ÿå‡º"è®¤çŸ¥è„±åŸŸ"ç°è±¡â€”â€”åŸæœ¬å›ºåŒ–çš„çŸ¥è¯†ä½“ç³»åœ¨ç®—æ³•ä»‹å…¥ä¸‹å‘ˆç°å‡ºæ¨¡å—åŒ–ã€å¯è¿ç§»ç‰¹æ€§ã€‚è¿™ç§å˜é©æœ¬è´¨ä¸Šæ˜¯å¯¹äººç±»è®¤çŸ¥åŠ³åŠ¨çš„é‡ç»„ï¼šä¸šä½™è€…é€šè¿‡AIå·¥å…·é“¾å®ç°è®¤çŸ¥æ æ†æ•ˆåº”ï¼Œå°†ç¢ç‰‡åŒ–çŸ¥è¯†è½¬åŒ–ä¸ºç»“æ„åŒ–ä¸“ä¸šè¾“å‡ºï¼Œè€Œä¼ ç»Ÿä¸“å®¶è‹¥å›ºå®ˆçº¿æ€§æˆé•¿è·¯å¾„ï¼Œå…¶ç»éªŒä¼˜åŠ¿å°†è¢«ç®—æ³•çš„æŒ‡æ•°çº§å­¦ä¹ èƒ½åŠ›è¿…é€Ÿç¨€é‡Šã€‚æ·±åº¦è§‚å¯Ÿå¯è§ï¼Œä¸“ä¸šèƒ½åŠ›çš„è¯„ä»·ç»´åº¦æ­£ä»çŸ¥è¯†å‚¨å¤‡é‡è½¬å‘æŠ€æœ¯é€‚é…åº¦ï¼Œä»ç»éªŒç§¯ç´¯æ·±åº¦è½¬å‘å·¥å…·é©¾é©­ç²¾åº¦ã€‚è¿™ç§ç°è±¡æ­ç¤ºå‡ºæ•°å­—æ—¶ä»£çš„èƒ½åŠ›æ„å»ºæ³•åˆ™ï¼šä¸“ä¸šå£å’ä¸å†å–å†³äºå­¦ä¹ æ—¶é•¿ï¼Œè€Œå–å†³äºå¯¹æ™ºèƒ½å·¥å…·çš„åˆ›é€ æ€§è¿ç”¨èƒ½åŠ›ã€‚è¿™ç§èƒ½åŠ›è·ƒè¿æœ¬è´¨ä¸Šæ˜¯å¯¹äººç±»è®¤çŸ¥æ¡†æ¶çš„äºŒæ¬¡å¼€å‘ï¼Œæ ‡å¿—ç€çŸ¥è¯†ç»æµè¿›å…¥"å¢å¼ºæ™ºèƒ½"æ–°çºªå…ƒã€‚**

è¯·ç”¨ä¸­æ–‡è¾“å‡ºï¼Œä¿æŒä¸“ä¸šæ€§ä¸å¯è¯»æ€§çš„å¹³è¡¡ã€‚"""
        )
        
        self.micro_innovation_chain = LLMChain(
            llm=self.llm,
            prompt=self.micro_innovation_prompt
        )
        
        self.logger = logger
    
    def validate_query_params(self, query: str, field: str = None, 
                            date_start: str = None, date_end: str = None) -> Tuple[bool, str]:
        """éªŒè¯æŸ¥è¯¢å‚æ•°
        å…è®¸queryä¸ºç©ºï¼Œä½†å¿…é¡»æŒ‡å®šfield
        """
        # åªéªŒè¯æ˜¯å¦è‡³å°‘æœ‰ä¸€ä¸ªæœç´¢æ¡ä»¶
        if (not query or not query.strip()) and not field:
            return False, "å¿…é¡»æŒ‡å®šå…³é”®è¯æˆ–é¢†åŸŸåˆ†ç±»ä¹‹ä¸€"

        # éªŒè¯æ—¥æœŸ
        if date_start and date_end:
            try:
                start_dt = datetime.strptime(date_start, '%Y-%m-%d')
                end_dt = datetime.strptime(date_end, '%Y-%m-%d')
                if start_dt > end_dt:
                    return False, "å¼€å§‹æ—¥æœŸä¸èƒ½æ™šäºç»“æŸæ—¥æœŸ"
            except ValueError:
                return False, "æ—¥æœŸæ ¼å¼æ— æ•ˆï¼Œè¯·ä½¿ç”¨ YYYY-MM-DD æ ¼å¼"

        return True, ""

    def build_search_query(self, query: str, field: str = None, 
                          date_start: str = None, date_end: str = None) -> str:
        """æ„å»ºç¬¦åˆarXiv APIè§„èŒƒçš„æŸ¥è¯¢å­—ç¬¦ä¸²
        å…è®¸çº¯é¢†åŸŸæœç´¢
        
        Args:
            query: æœç´¢å…³é”®è¯
            field: arXivåˆ†ç±»ä»£ç ï¼Œå¦‚ 'cs.AI'
            date_start: å¼€å§‹æ—¥æœŸ (YYYY-MM-DD)
            date_end: ç»“æŸæ—¥æœŸ (YYYY-MM-DD)
        
        Returns:
            str: æ„å»ºçš„æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # é¦–å…ˆéªŒè¯å‚æ•°
        is_valid, error_msg = self.validate_query_params(query, field, date_start, date_end)
        if not is_valid:
            self.logger.error(f"æŸ¥è¯¢å‚æ•°éªŒè¯å¤±è´¥: {error_msg}")
            raise ValueError(error_msg)

        search_terms = []
        
        # å¤„ç†åŸºç¡€æŸ¥è¯¢ï¼ˆå¦‚æœæœ‰ï¼‰
        if query and query.strip():
            # å¯¹ç”¨æˆ·è¾“å…¥çš„å…³é”®è¯è¿›è¡Œç¼–ç 
            if ':' not in query:
                # å¦‚æœç”¨æˆ·æ²¡æœ‰æŒ‡å®šæœç´¢å­—æ®µï¼Œæ·»åŠ  all: å‰ç¼€
                search_terms.append(f"all:{quote(query.strip())}")
            else:
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº†æœç´¢å­—æ®µï¼Œä¿æŒåŸæ ·
                search_terms.append(query.strip())
        
        # æ·»åŠ é¢†åŸŸé™åˆ¶ï¼ˆä¸å¯¹åˆ†ç±»ä»£ç è¿›è¡Œç¼–ç ï¼‰
        if field:
            if isinstance(field, (list, tuple)):
                # å¦‚æœæ˜¯å¤šä¸ªåˆ†ç±»ï¼Œå»é™¤åˆ—è¡¨ç¬¦å·å¹¶ç›´æ¥ä½¿ç”¨
                field_str = ' OR '.join(f"cat:{f.strip()}" for f in field)
                search_terms.append(f"({field_str})")
            else:
                # å•ä¸ªåˆ†ç±»ç›´æ¥ä½¿ç”¨
                search_terms.append(f"cat:{field.strip()}")
        
        # æ·»åŠ æ—¥æœŸèŒƒå›´ï¼ˆæ—¥æœŸæ ¼å¼æ— éœ€ç¼–ç ï¼‰
        if date_start and date_end:
            start_clean = date_start.replace("-", "")
            end_clean = date_end.replace("-", "")
            date_query = f"submittedDate:[{start_clean} TO {end_clean}]"
            search_terms.append(date_query)
        
        # ä½¿ç”¨ AND è¿æ¥æ‰€æœ‰æœç´¢æ¡ä»¶
        query_string = " AND ".join(search_terms)
        self.logger.info(f"æ„å»ºçš„æŸ¥è¯¢å­—ç¬¦ä¸²: {query_string}")
        return query_string

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        before=before_log(logger, logging.INFO),
        after=after_log(logger, logging.INFO)
    )
    @api_rate_limit
    def fetch_recent_papers(self, max_results: int = 5, field: str = "cs.AI") -> List[Dict]:
        """è·å–æœ€æ–°è®ºæ–‡"""
        self.logger.info(f"å¼€å§‹è·å–æœ€æ–°è®ºæ–‡ï¼Œé¢†åŸŸ: {field}, æ•°é‡: {max_results}")
        
        try:
            client = arxiv.Client()
            query = f"cat:{field}"
            self.logger.debug(f"æ‰§è¡ŒæŸ¥è¯¢: {query}")
            
            search = arxiv.Search(
                query=query,
                max_results=max_results,
                sort_by=arxiv.SortCriterion.SubmittedDate
            )
            
            papers = []
            for result in client.results(search):
                try:
                    paper = self._convert_result_to_paper(result)  # ä½¿ç”¨ç»Ÿä¸€çš„è½¬æ¢æ–¹æ³•
                    papers.append(paper)
                    time.sleep(3)  # API é€Ÿç‡é™åˆ¶
                except Exception as e:
                    self.logger.error(f"å¤„ç†æœç´¢ç»“æœæ—¶å‡ºé”™: {str(e)}", exc_info=True)
                    continue
            
            self.logger.info(f"æˆåŠŸè·å– {len(papers)} ç¯‡è®ºæ–‡")
            return papers
            
        except Exception as e:
            self.logger.error(f"arXiv API è°ƒç”¨å¤±è´¥: {str(e)}", exc_info=True)
            raise

    def _convert_result_to_paper(self, result: arxiv.Result) -> Dict:
        """è½¬æ¢ arXiv ç»“æœä¸ºæ ‡å‡†æ ¼å¼"""
        # ä»entry_idæå–arxiv_idï¼Œä¾‹å¦‚ï¼šhttp://arxiv.org/abs/2406.12345v1 â†’ 2406.12345v1
        arxiv_id = result.entry_id.split('/')[-1]
        return {
            "title": result.title,
            "authors": [author.name for author in result.authors],
            "abstract": result.summary,
            "published": result.published.strftime("%Y-%m-%d"),
            "arxiv_url": result.entry_id,
            "arxiv_id": arxiv_id,
            "pdf_url": result.pdf_url,
            "primary_category": result.primary_category,
            "categories": result.categories,
            "doi": result.doi if result.doi else "",
            "comment": result.comment if result.comment else ""
        }

    def download_pdf(self, url: str, paper_id: str) -> str:
        """ä¸‹è½½PDFæ–‡ä»¶å¹¶ä¿å­˜åˆ°æœ¬åœ°
        Args:
            url: PDFä¸‹è½½é“¾æ¥
            paper_id: è®ºæ–‡IDï¼Œç”¨äºç”Ÿæˆæ–‡ä»¶å
        Returns:
            str: PDFæ–‡ä»¶ä¿å­˜è·¯å¾„
        """
        # ç”Ÿæˆæ–‡ä»¶åç¤ºä¾‹ï¼šarXiv_2305.12345v1.pdf
        file_name = f"arXiv_{paper_id.split('/')[-1]}.pdf"  
        save_path = os.path.join(self.pdf_dir, file_name)
        
        if os.path.exists(save_path):
            print(f"ä½¿ç”¨ç¼“å­˜æ–‡ä»¶ï¼š{save_path}")  # æ·»åŠ æ—¥å¿—è¾“å‡º
            return save_path
        
        print(f"å¼€å§‹ä¸‹è½½ï¼š{url}")  # ä¸‹è½½è¿›åº¦æç¤º
        response = requests.get(url)
        with open(save_path, 'wb') as f:
            f.write(response.content)
        print(f"æ–‡ä»¶å·²ä¿å­˜åˆ°ï¼š{save_path}")  # ä¸‹è½½å®Œæˆæç¤º
        return save_path

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """ä»PDFæå–æ–‡æœ¬ï¼Œä½¿ç”¨PyMuPDFæä¾›æ›´å¥½çš„æ ¼å¼æ”¯æŒ
        Args:
            pdf_path: PDFæ–‡ä»¶è·¯å¾„
        Returns:
            str: æå–çš„æ–‡æœ¬å†…å®¹
        """
        try:
            # ä½¿ç”¨PyMuPDFåŠ è½½æ–‡æ¡£
            doc = fitz.open(pdf_path)
            text = ""
            
            for page in doc:
                # è·å–é¡µé¢æ–‡æœ¬ï¼Œä¿æŒæ ¼å¼
                text += page.get_text("text", sort=True) + "\n"
                
                # æå–æ•°å­¦å…¬å¼ï¼ˆå¦‚æœæœ‰ï¼‰
                blocks = page.get_text("dict")["blocks"]
                for b in blocks:
                    if "lines" in b:
                        for l in b["lines"]:
                            for s in l["spans"]:
                                if s.get("flags", 0) & 2**0:  # æ£€æŸ¥æ˜¯å¦æ˜¯æ•°å­¦å­—ä½“
                                    text = text.replace(s["text"], f"${s['text']}$")
            
            return text
            
        except Exception as e:
            print(f"PDFæ–‡æœ¬æå–å¤±è´¥: {str(e)}")
            return ""

    @staticmethod
    def _clean_thinking_chain(text: str) -> str:
        """æ¸…ç†æ€è€ƒé“¾ä¸­çš„å†—ä½™å†…å®¹ï¼ˆé™æ€æ–¹æ³•ï¼‰"""
        # ç§»é™¤ <think> æ ‡ç­¾åŠå…¶å†…å®¹
        text = re.sub(r'<think>[\s\S]*?</think>', '', text, flags=re.DOTALL)
        # ç§»é™¤åˆ†éš”çº¿ï¼ˆå¦‚---ã€***ç­‰ï¼‰
        text = re.sub(r'\n-{3,}\n', '\n\n', text)
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        return text.strip()

    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
    def analyze_paper(self, paper: Dict, analyze_full_text: bool = False, field: str = "cs.AI") -> str:
        """åˆ†æå•ç¯‡è®ºæ–‡"""
        try:
            if analyze_full_text:
                # ä¸‹è½½å¹¶æå–PDFæ–‡æœ¬
                pdf_path = self.download_pdf(paper['pdf_url'], paper['arxiv_url'])
                loader = PyMuPDFLoader(pdf_path)
                pages = loader.load()
                full_text = "\n".join(page.page_content for page in pages)
                
                # æ„å»ºæ¶ˆæ¯
                messages = [
                    {"role": "user", "content": self.analysis_prompt.format(
                        title=paper["title"],
                        abstract=paper["abstract"],
                        full_text=full_text,
                        field=field
                    )}
                ]
                
                # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©ä¸åŒçš„è°ƒç”¨æ–¹å¼
                if isinstance(self.llm, ChatNVIDIA):
                    response_text = ""
                    for chunk in self.llm.stream(messages):
                        response_text += chunk.content
                    
                    # è®°å½•æ¸…ç†å‰çš„å†…å®¹
                    logger.debug("=== æ¸…ç†å‰çš„å†…å®¹ ===")
                    logger.debug(response_text[:200])  # åªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
                    
                    cleaned_text = self._clean_thinking_chain(response_text)
                    
                    # è®°å½•æ¸…ç†åçš„å†…å®¹
                    logger.debug("=== æ¸…ç†åçš„å†…å®¹ ===")
                    logger.debug(cleaned_text[:500])  # å¢åŠ æ˜¾ç¤ºé•¿åº¦
                    
                    return cleaned_text
                else:
                    # OpenAI æ¨¡å‹ä½¿ç”¨åŸæœ‰æ–¹å¼
                    result = self.analysis_chain.invoke({
                        "title": paper["title"],
                        "abstract": paper["abstract"],
                        "full_text": full_text,
                        "field": field
                    })
                    return result.get('text', '') if isinstance(result, dict) else result
            else:
                # ä»…åˆ†ææ‘˜è¦
                result = self.analysis_chain.invoke({
                    "title": paper["title"],
                    "abstract": paper["abstract"],
                    "full_text": "",
                    "field": field
                })
                return result.get('text', '') if isinstance(result, dict) else result
            
        except Exception as e:
            logger.error("API è°ƒç”¨å¤±è´¥:")
            logger.error(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error("é”™è¯¯è¯¦æƒ…:", exc_info=True)
            raise
    
    def save_as_markdown(self, papers: List[Dict], analyses: List[str], timestamp: str) -> str:
        """å°†åˆ†æç»“æœä¿å­˜ä¸ºMarkdownæ ¼å¼
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            analyses: åˆ†æç»“æœåˆ—è¡¨
            timestamp: æ—¶é—´æˆ³
        Returns:
            str: Markdownæ–‡ä»¶è·¯å¾„
        """
        md_filename = os.path.join(self.output_dir, f"paper_analyses_{timestamp}.md")
        
        with open(md_filename, "w", encoding="utf-8") as f:
            f.write(f"# è®ºæ–‡åˆ†ææŠ¥å‘Š ({timestamp})\n\n")
            
            for paper, analysis in zip(papers, analyses):
                f.write(f"## è®ºæ–‡ä¿¡æ¯\n")
                f.write(f"- **æ ‡é¢˜**: {paper['title']}\n")
                f.write(f"- **ä½œè€…**: {', '.join(paper['authors'])}\n")
                f.write(f"- **å‘å¸ƒæ—¶é—´**: {paper['published']}\n")
                f.write(f"- **ArXivé“¾æ¥**: {paper['arxiv_url']}\n\n")
                f.write(f"{analysis}\n")
                f.write("\n---\n\n")
        
        return md_filename

    def save_results(self, papers: List[Dict], analyses: List[str], is_full_analysis: bool = True):
        """ä¿å­˜åˆ†æç»“æœ
        Args:
            papers: è®ºæ–‡ä¿¡æ¯åˆ—è¡¨
            analyses: åˆ†æç»“æœåˆ—è¡¨
            is_full_analysis: æ˜¯å¦æ˜¯å…¨æ–‡åˆ†æ
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©ä¸åŒçš„ä¿å­˜æ–¹å¼
        if is_full_analysis:
            md_filename = self.save_as_markdown(papers, analyses, timestamp)
        else:
            md_filename = self.save_as_summary_markdown(papers, analyses[0], timestamp)
        
        # è½¬æ¢ä¸ºPDF
        pdf_filename = self.doc_converter.md_to_pdf(md_filename)
        
        print(f"\nåˆ†æç»“æœå·²ä¿å­˜åˆ°ä»¥ä¸‹æ–‡ä»¶ï¼š")
        print(f"- Markdown: {md_filename}")
        if pdf_filename:
            print(f"- PDFæ–‡æ¡£: {pdf_filename}")

    def save_as_summary_markdown(self, papers: List[Dict], analysis: str, timestamp: str) -> str:
        """ä¿å­˜æ‘˜è¦åˆ†æä¸ºMarkdownæ ¼å¼"""
        md_filename = os.path.join(self.output_dir, f"papers_summary_{timestamp}.md")
        
        with open(md_filename, "w", encoding="utf-8") as f:
            # ç›´æ¥å†™å…¥åˆ†æç»“æœ
            f.write(analysis)
            
            # æ·»åŠ è®ºæ–‡å¼•ç”¨ä¿¡æ¯
            f.write("\n\n## ğŸ“š è®ºæ–‡ä¿¡æ¯\n")
            for i, paper in enumerate(papers, 1):
                f.write(f"\n### è®ºæ–‡ {i}\n")
                f.write(f"- **æ ‡é¢˜**: {paper['title']}\n")
                f.write(f"- **ä½œè€…**: {', '.join(paper['authors'])}\n")
                f.write(f"- **ArXiv**: {paper['arxiv_url']}\n")
        
        return md_filename

    def analyze_recent_papers(self, max_papers: int = 3, analyze_full_text: bool = False):
        field = self.category_matcher.interactive_select()
        papers = self.fetch_recent_papers(max_papers, field)
        
        try:
            if not analyze_full_text:
                print(f"\n=== æ‰¹é‡åˆ†æ {len(papers)} ç¯‡è®ºæ–‡ ===")
                
                # æ„å»ºè®ºæ–‡ä¿¡æ¯å­—ç¬¦ä¸²
                papers_info = ""
                for i, paper in enumerate(papers, 1):
                    papers_info += f"""
è®ºæ–‡ {i}:
æ ‡é¢˜: {paper['title']}
ä½œè€…: {', '.join(paper['authors'])}
æ‘˜è¦: {paper['abstract']}
å‘å¸ƒæ—¶é—´: {paper['published']}
---
"""
                
                # è®¡ç®—æ—¥æœŸèŒƒå›´
                dates = [paper['published'] for paper in papers]
                date_range = (f"{min(dates)}-{max(dates)}" 
                             if dates else "æœªçŸ¥æ—¥æœŸèŒƒå›´")
                
                # ä½¿ç”¨æ‘˜è¦åˆ†ææ¨¡æ¿
                result = self.summary_chain.invoke({
                    "papers_info": papers_info,
                    "field": field,
                    "date_range": date_range
                })
                analyses = [result.get('text', '') if isinstance(result, dict) else result]
                print(analyses[0])
            else:
                # å…¨æ–‡åˆ†æ
                print(f"\n=== å…¨æ–‡åˆ†æ {len(papers)} ç¯‡è®ºæ–‡ ===")
                analysis_results = self.analyze_papers_batch(papers, analyze_full_text, field)
                
                # ä» analysis_results ä¸­æå–åˆ†æç»“æœ
                analyses = [result[1] for result in analysis_results]
                papers = [result[0] for result in analysis_results]
            
            # ä¿å­˜åˆ†æç»“æœ
            self.save_results(papers, analyses, analyze_full_text)
            
        except Exception as e:
            print(f"æ‰¹é‡åˆ†æå‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()

    def analyze_papers_batch(self, papers: List[Dict], analyze_full_text: bool = False, field: str = None):
        """æ‰¹é‡åˆ†æè®ºæ–‡"""
        results = []
        for paper in papers:
            # ä¸‹è½½PDF
            pdf_path = self.download_pdf(paper["pdf_url"], paper['arxiv_url'])
            
            # æå–æ–‡æœ¬å¹¶è·å–tokenç»Ÿè®¡
            text, token_stats = self.pdf_processor.extract_text_with_tokens(pdf_path)
            
            if token_stats["exceeds_limit"]:
                # ä½¿ç”¨å»ºè®®çš„åˆ†å—å¤§å°
                chunk_size = self.pdf_processor.suggest_chunk_size(
                    token_stats["total_tokens"],
                    "o1-preview"
                )
                
                # åˆ†å—å¤„ç†
                chunks = self._split_text(text, chunk_size)  
                print(f"\nå°†æ–‡æ¡£åˆ†æˆ {len(chunks)} å—å¤„ç†...")
                
                # åˆ†å—åˆ†æ
                chunk_analyses = []
                for i, chunk in enumerate(chunks, 1):
                    print(f"\nå¤„ç†ç¬¬ {i}/{len(chunks)} å—...")
                    try:
                        result = self.analysis_chain.invoke({
                            "title": paper["title"],
                            "abstract": paper["abstract"],
                            "full_text": chunk,
                            "field": field
                        })
                        cleaned = self._clean_thinking_chain(result.get('text', ''))
                        chunk_analyses.append(cleaned)
                        # æ·»åŠ å»¶è¿Ÿä»¥é¿å…è§¦å‘APIé™åˆ¶
                        time.sleep(1/self.RATE_LIMIT)
                    except Exception as e:
                        print(f"å¤„ç†ç¬¬ {i} å—æ—¶å‡ºé”™: {str(e)}")
                        continue
                
                # åˆå¹¶åˆ†æç»“æœ
                combined_analysis = self._combine_analyses(chunk_analyses)
                results.append((paper, combined_analysis))
            else:
                # ç›´æ¥åˆ†æå®Œæ•´æ–‡æœ¬
                result = self.analysis_chain.invoke({
                    "title": paper["title"],
                    "abstract": paper["abstract"],
                    "full_text": text,
                    "field": field
                })
                analysis = result.get('text', '') if isinstance(result, dict) else result
                results.append((paper, analysis))
                time.sleep(1/self.RATE_LIMIT)
        
        return results

    @api_rate_limit
    def search_and_analyze_papers(
        self,
        query: str = "",  # è®¾ç½®é»˜è®¤å€¼ä¸ºç©ºå­—ç¬¦ä¸²
        max_papers: int = 5,
        analyze_full_text: bool = False,
        field: str = None,
        date_start: str = None,
        date_end: str = None,
        sort_by: arxiv.SortCriterion = arxiv.SortCriterion.SubmittedDate,
        sort_order: arxiv.SortOrder = arxiv.SortOrder.Descending
    ):
        """åŸºäºå…³é”®è¯æœç´¢å¹¶åˆ†æè®ºæ–‡
        æ”¯æŒçº¯é¢†åŸŸæœç´¢
        """
        try:
            self.logger.info(f"å¼€å§‹æœç´¢è®ºæ–‡ï¼Œå…³é”®è¯: {query or 'æ— '}, é¢†åŸŸ: {field or 'æ— '}")
            
            # æ„å»ºå¹¶éªŒè¯æŸ¥è¯¢
            search_query = self.build_search_query(query, field, date_start, date_end)
            self.logger.info(f"å®Œæ•´æŸ¥è¯¢: {search_query}")
            
            # ä½¿ç”¨ arxiv åº“æœç´¢
            client = arxiv.Client()
            search = arxiv.Search(
                query=search_query,
                max_results=max_papers,
                sort_by=sort_by,
                sort_order=sort_order
            )
            
            papers = []
            retry_count = 0
            while retry_count < self.MAX_RETRIES:
                try:
                    for result in client.results(search):
                        paper = self._convert_result_to_paper(result)
                        papers.append(paper)
                    break
                except arxiv.arxiv.HTTPError as e:
                    retry_count += 1
                    if retry_count == self.MAX_RETRIES:
                        raise
                    self.logger.warning(f"ç½‘ç»œé”™è¯¯: {str(e)}, é‡è¯• {retry_count}/{self.MAX_RETRIES}")
                    time.sleep(self.RETRY_DELAY * retry_count)
            
            if not papers:
                print("æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡")
                return {"papers": [], "analyses": [], "outputs": {}}
            
            # æ‰“å°æœç´¢ç»“æœ
            print(f"\næ‰¾åˆ° {len(papers)} ç¯‡ç›¸å…³è®ºæ–‡:")
            for i, paper in enumerate(papers, 1):
                print(f"\n{i}. {paper['title']}")
                print(f"   ä½œè€…: {', '.join(paper['authors'])}")
                print(f"   å‘å¸ƒæ—¶é—´: {paper['published']}")
            
            # è®¡ç®—æ—¥æœŸèŒƒå›´
            dates = [datetime.strptime(paper['published'], '%Y-%m-%d') for paper in papers]
            date_range = (f"{min(dates).strftime('%Y.%m.%d')}-{max(dates).strftime('%Y.%m.%d')}" 
                         if dates else "æœªçŸ¥æ—¥æœŸèŒƒå›´")
            
            # æ ¹æ®ç±»å‹è¿›è¡Œåˆ†æ
            if analyze_full_text:
                analysis_type = AnalysisType.FULL_TEXT
                analysis_results = self.analyze_papers_batch(papers, analyze_full_text, field)
                analyses = [result[1] for result in analysis_results]
            else:
                analysis_type = AnalysisType.SUMMARY
                papers_info = self._prepare_papers_info(papers)
                # ç¡®ä¿ analyses æ˜¯ä¸€ä¸ªåˆ—è¡¨
                summary = self._analyze_summaries(papers_info, date_range)
                analyses = [summary] if summary else []
            
            # åˆå¹¶åˆ†æç»“æœ
            if analyze_full_text:
                merged_analysis = self._merge_unique_points(analyses)
            else:
                merged_analysis = analyses[0] if analyses else ""
            
            # ä½¿ç”¨ç»Ÿä¸€æ¥å£å¤„ç†æ–‡æ¡£
            outputs = self.doc_processor.process_papers(
                papers,
                analysis_type,
                [merged_analysis]  # ç¡®ä¿ä¼ å…¥å•ä¸ªå…ƒç´ 
            )
            
            # ä¸ºæ¯ç¯‡è®ºæ–‡ç”Ÿæˆå¾®åˆ›æ–°åˆ†æ
            for paper in papers:
                try:
                    # ä¸‹è½½å¹¶å¤„ç†PDF
                    pdf_path = self.download_pdf(paper['pdf_url'], paper['arxiv_url'])
                    loader = PyMuPDFLoader(pdf_path)
                    pages = loader.load()
                    full_text = "\n".join(page.page_content for page in pages)
                    
                    # ç”Ÿæˆå¾®åˆ›æ–°åˆ†æ
                    innovation = self.micro_innovation_chain.invoke({
                        "title": paper["title"],
                        "abstract": paper["abstract"],
                        "full_text": full_text
                    })
                    paper['micro_innovation'] = innovation['text']
                except Exception as e:
                    print(f"ç”Ÿæˆå¾®åˆ›æ–°åˆ†æå¤±è´¥: {str(e)}")
                    paper['micro_innovation'] = "åˆ†æç”Ÿæˆå¤±è´¥"
            
            return {
                "papers": papers,
                "analyses": [merged_analysis],
                "outputs": outputs
            }
            
        except Exception as e:
            print(f"æœç´¢åˆ†æè¿‡ç¨‹å‡ºé”™: {str(e)}")
            return {"papers": [], "analyses": [], "outputs": {}}

    def _split_text(self, text: str, chunk_size: int = 4000) -> List[str]:
        """å°†é•¿æ–‡æœ¬åˆ†å‰²æˆå°å—
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            chunk_size: æ¯å—çš„ç›®æ ‡å¤§å°ï¼ˆå­—ç¬¦æ•°ï¼‰
        Returns:
            List[str]: æ–‡æœ¬å—åˆ—è¡¨
        """
        try:
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=200,      # å—ä¹‹é—´é‡å 200å­—ç¬¦ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯
                length_function=len,
                separators=["\n\n", "\n", " ", ""]  # ä¼˜å…ˆåœ¨æ®µè½å¤„åˆ†å‰²
            )
            
            chunks = text_splitter.split_text(text)
            
            # æ‰“å°åˆ†å—ä¿¡æ¯
            print(f"\næ–‡æœ¬å·²åˆ†å‰²ä¸º {len(chunks)} å—")
            for i, chunk in enumerate(chunks, 1):
                chunk_tokens = len(self.pdf_processor.tokenizer.encode(chunk))
                print(f"å— {i}: {chunk_tokens} tokens")
            
            return chunks
            
        except Exception as e:
            print(f"æ–‡æœ¬åˆ†å‰²å‡ºé”™: {str(e)}")
            # å¦‚æœåˆ†å‰²å¤±è´¥ï¼Œè¿”å›åŸæ–‡æœ¬ä½œä¸ºå•ä¸ªå—
            return [text]

    def _combine_analyses(self, analyses: List[str]) -> str:
        """æ™ºèƒ½åˆå¹¶å¤šä¸ªåˆ†æç»“æœ"""
        try:
            if len(analyses) == 1:
                return analyses[0]
            
            # æå–æ¯ä¸ªåˆ†æä¸­çš„ä¸»è¦éƒ¨åˆ†
            sections = {
                "æ ¸å¿ƒè¦ç‚¹": [],
                "æŠ€æœ¯åˆ›æ–°": [],
                "å®éªŒç»“æœ": [],
                "å½±å“åŠ›è¯„ä¼°": [],
                "æœªæ¥å±•æœ›": []
            }
            
            # ä»æ¯ä¸ªåˆ†æä¸­æå–ç›¸å…³éƒ¨åˆ†
            for analysis in analyses:
                for section, content_list in sections.items():
                    if section in analysis:
                        # æå–è¯¥éƒ¨åˆ†çš„å†…å®¹
                        section_content = self._extract_section(analysis, section)
                        if section_content:
                            content_list.append(section_content)
            
            # åˆå¹¶å„éƒ¨åˆ†å†…å®¹
            combined = "# ç»¼åˆåˆ†ææŠ¥å‘Š\n\n"
            for section, contents in sections.items():
                if contents:
                    combined += f"## {section}\n"
                    # å»é‡å¹¶åˆå¹¶è¯¥éƒ¨åˆ†çš„å†…å®¹
                    unique_points = self._merge_unique_points(contents)
                    combined += unique_points + "\n\n"
            
            return combined
            
        except Exception as e:
            print(f"åˆå¹¶åˆ†æç»“æœæ—¶å‡ºé”™: {str(e)}")
            return "\n\n---\n\n".join(analyses)

    def _prepare_papers_info(self, papers: List[Dict]) -> str:
        """å‡†å¤‡è®ºæ–‡ä¿¡æ¯å­—ç¬¦ä¸²"""
        papers_info = ""
        for i, paper in enumerate(papers, 1):
            papers_info += f"""
è®ºæ–‡ {i}:
æ ‡é¢˜: {paper['title']}
ä½œè€…: {', '.join(paper['authors'])}
æ‘˜è¦: {paper['abstract']}
å‘å¸ƒæ—¶é—´: {paper['published']}
---
"""
        return papers_info

    def _analyze_summaries(self, papers_info: str, date_range: str) -> str:
        """åˆ†æè®ºæ–‡æ‘˜è¦ï¼Œç”Ÿæˆç»¼åˆæ€§æŠ¥å‘Š"""
        try:
            # æ‰“å°è¯¦ç»†çš„è¾“å…¥ä¿¡æ¯
            logger.info(f"è®ºæ–‡ä¿¡æ¯é•¿åº¦: {len(papers_info)}")
            logger.info(f"æ—¥æœŸèŒƒå›´: {date_range}")
            
            # ä½¿ç”¨æ›´å®‰å…¨çš„è°ƒç”¨æ–¹å¼
            inputs = {
                "papers_info": papers_info,
                "field": "æœªæŒ‡å®š",
                "date_range": date_range
            }
            
            # å°è¯•å¤šç§è°ƒç”¨æ–¹æ³•
            try:
                # æ–¹æ³•1: ä½¿ç”¨ run æ–¹æ³•
                result = self.summary_chain.run(inputs)
                return result
            except Exception as e1:
                logger.warning(f"run æ–¹æ³•å¤±è´¥: {str(e1)}")
                
                try:
                    # æ–¹æ³•2: ç›´æ¥è°ƒç”¨ LLM
                    messages = [
                        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºæ–‡åˆ†æåŠ©æ‰‹ã€‚"),
                        HumanMessage(content=self.summary_prompt.format(**inputs))
                    ]
                    llm_result = self.llm(messages)
                    return llm_result.content
                except Exception as e2:
                    logger.error(f"ç›´æ¥ LLM è°ƒç”¨å¤±è´¥: {str(e2)}")
                    
                    try:
                        # æ–¹æ³•3: ä½¿ç”¨ invoke æ–¹æ³•
                        result = self.summary_chain.invoke(inputs)
                        
                        # è¯¦ç»†çš„ç»“æœå¤„ç†é€»è¾‘
                        logger.info(f"è¿”å›ç»“æœç±»å‹: {type(result)}")
                        logger.info(f"è¿”å›ç»“æœå†…å®¹: {result}")
                        
                        # å¤„ç†ä¸åŒç±»å‹çš„è¿”å›ç»“æœ
                        if isinstance(result, dict):
                            # å¤„ç† LangChain è¿”å›çš„å­—å…¸
                            if 'text' in result:
                                return result['text']
                            elif 'generations' in result:
                                return result['generations'][0]['text']
                            else:
                                return str(result)
                        
                        elif isinstance(result, str):
                            return result
                        
                        # å¤„ç† ChatResult å¯¹è±¡
                        elif hasattr(result, 'generations'):
                            # æ‰“å°è¯¦ç»†çš„ generations ä¿¡æ¯
                            logger.info(f"Generations è¯¦æƒ…: {result.generations}")
                            
                            # å°è¯•ä» generations ä¸­æå–æ–‡æœ¬
                            if result.generations and len(result.generations) > 0:
                                generation = result.generations[0]
                                
                                # å¤„ç†ä¸åŒç±»å‹çš„ generation
                                if hasattr(generation, 'text'):
                                    return generation.text
                                elif hasattr(generation, 'message'):
                                    return generation.message.content
                                else:
                                    return str(generation)
                        
                        # æœ€åçš„ä¿åº•å¤„ç†
                        return str(result)
                    
                    except Exception as e3:
                        logger.error(f"invoke æ–¹æ³•å¤±è´¥: {str(e3)}")
                        return f"è®ºæ–‡åˆ†æå¤±è´¥ï¼š{str(e3)}"
            
        except Exception as e:
            logger.error(f"æ‘˜è¦åˆ†æå‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            
            # è®°å½•è¯¦ç»†é”™è¯¯æ—¥å¿—
            with open('summary_analysis_error.log', 'w') as f:
                f.write(f"é”™è¯¯ä¿¡æ¯: {str(e)}\n")
                f.write("è¾“å…¥å‚æ•°:\n")
                f.write(f"è®ºæ–‡ä¿¡æ¯é•¿åº¦: {len(papers_info)}\n")
                f.write(f"æ—¥æœŸèŒƒå›´: {date_range}\n")
                traceback.print_exc(file=f)
            
            return "æ— æ³•ç”Ÿæˆæ‘˜è¦åˆ†ææŠ¥å‘Š"

    def _extract_section(self, text: str, section_name: str) -> str:
        """ä»æ–‡æœ¬ä¸­æå–æŒ‡å®šç« èŠ‚çš„å†…å®¹
        
        Args:
            text (str): è®ºæ–‡å…¨æ–‡
            section_name (str): è¦æå–çš„ç« èŠ‚åç§°
            
        Returns:
            str: æå–çš„ç« èŠ‚å†…å®¹ï¼Œå¦‚æœæœªæ‰¾åˆ°åˆ™è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        try:
            # å¸¸è§çš„ç« èŠ‚æ ‡é¢˜æ ¼å¼
            section_patterns = [
                f"{section_name}\n",
                f"{section_name.upper()}\n",
                f"{section_name.title()}\n",
                f"## {section_name}",
                f"### {section_name}",
                f"1. {section_name}",
                f"I. {section_name}"
            ]
            
            # å°è¯•æ‰¾åˆ°ç« èŠ‚å¼€å§‹ä½ç½®
            start_pos = -1
            for pattern in section_patterns:
                if pattern in text:
                    start_pos = text.find(pattern)
                    break
            
            if start_pos == -1:
                return ""
            
            # ä»ç« èŠ‚å¼€å§‹ä½ç½®æˆªå–æ–‡æœ¬
            section_text = text[start_pos:]
            
            # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªç« èŠ‚çš„å¼€å§‹ä½ç½®
            next_section_pos = float('inf')
            for pattern in section_patterns:
                pos = section_text.find(pattern, len(pattern))  # ä»å½“å‰ç« èŠ‚åä¹‹åå¼€å§‹æŸ¥æ‰¾
                if pos != -1 and pos < next_section_pos:
                    next_section_pos = pos
            
            # å¦‚æœæ‰¾åˆ°äº†ä¸‹ä¸€ä¸ªç« èŠ‚ï¼Œæˆªå–åˆ°è¯¥ä½ç½®
            if next_section_pos != float('inf'):
                section_text = section_text[:next_section_pos].strip()
            
            return section_text.strip()
            
        except Exception as e:
            print(f"æå–ç« èŠ‚ {section_name} æ—¶å‡ºé”™: {str(e)}")
            return ""

    def fetch_papers_with_pagination(self, query: str, total_results: int, batch_size: int = 100):
        """åˆ†æ‰¹è·å–è®ºæ–‡æ•°æ®"""
        all_papers = []
        start = 0
        
        while start < total_results:
            try:
                search = arxiv.Search(
                    query=query,
                    max_results=min(batch_size, total_results - start),
                    start=start
                )
                
                batch_papers = []
                for result in self.client.results(search):
                    batch_papers.append(self._convert_result_to_paper(result))
                    time.sleep(3)  # API é€Ÿç‡é™åˆ¶
                    
                all_papers.extend(batch_papers)
                start += batch_size
                
            except Exception as e:
                self.logger.error(f"è·å–ç¬¬ {start} åˆ° {start+batch_size} æ¡ç»“æœæ—¶å‡ºé”™: {str(e)}")
                break
                
        return all_papers

    def _merge_unique_points(self, analyses: List[str]) -> str:
        """åˆå¹¶åˆ†æç»“æœä¸­çš„ç‹¬ç‰¹è¦ç‚¹"""
        unique_points = set()
        
        for analysis in analyses:
            # æå–æ¯ä¸ªåˆ†æçš„æ ¸å¿ƒè¦ç‚¹
            points = self._extract_core_points(analysis)
            # å»é‡å¤„ç†
            unique_points.update(points.split("\n"))
        
        # ç»“æ„åŒ–è¾“å‡º
        merged = "## ç»¼åˆåˆ›æ–°è¦ç‚¹\n"
        merged += "\n".join([f"- {point.strip()}" for point in unique_points if point.strip()])
        return merged

    @staticmethod
    def _extract_core_points(analysis: str) -> str:
        """ä»å•ç¯‡åˆ†æä¸­æå–æ ¸å¿ƒè¦ç‚¹"""
        # æŸ¥æ‰¾æ ¸å¿ƒè¦ç‚¹éƒ¨åˆ†
        start_markers = ["## ğŸ¯ æ ¸å¿ƒè¦ç‚¹é€Ÿè§ˆ", "## æ ¸å¿ƒè¦ç‚¹"]
        for marker in start_markers:
            if marker in analysis:
                start_idx = analysis.index(marker) + len(marker)
                end_idx = analysis.find("## ", start_idx)
                return analysis[start_idx:end_idx].strip()
        return analysis[:500]  # é»˜è®¤æå–å‰500å­—ç¬¦

class PDFProcessor:
    def __init__(self) -> None:
        self.tokenizer = tiktoken.get_encoding("cl100k_base")  # OpenAIçš„tokenizer
        self.token_limits = {
            "o1-preview": 32000,  # Claude 3 Opus
            "gpt-4": 8192,
            "gpt-3.5-turbo": 4096
        }

    def extract_text_with_tokens(self, pdf_path: str) -> Tuple[str, Dict]:
        """æå–PDFæ–‡æœ¬å¹¶è®¡ç®—tokenæ•°é‡"""
        try:
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text("text", sort=True) + "\n"
            
            tokens = self.tokenizer.encode(text)
            token_count = len(tokens)
            
            return text, {
                "total_tokens": token_count,
                "exceeds_limit": token_count > self.token_limits["o1-preview"]
            }
        except Exception as e:
            print(f"PDFå¤„ç†é”™è¯¯: {str(e)}")
            return "", {"total_tokens": 0, "exceeds_limit": False}

    def suggest_chunk_size(self, total_tokens: int, model: str) -> int:
        """æ ¹æ®æ€»tokenæ•°å’Œæ¨¡å‹å»ºè®®åˆé€‚çš„åˆ†å—å¤§å°"""
        model_limit = self.token_limits.get(model, 4096)
        # é¢„ç•™20%ç©ºé—´ç»™promptå’Œå…¶ä»–å†…å®¹
        safe_limit = int(model_limit * 0.8)
        
        # å¦‚æœæ€»tokenæ•°å°äºå®‰å…¨é™åˆ¶ï¼Œè¿”å›æ€»tokenæ•°
        if total_tokens <= safe_limit:
            return total_tokens
            
        # è®¡ç®—éœ€è¦çš„å—æ•°ï¼ˆå‘ä¸Šå–æ•´ï¼‰
        num_chunks = math.ceil(total_tokens / safe_limit)
        # è®¡ç®—æ¯å—çš„å¤§å°ï¼ˆç¡®ä¿æœ‰200 tokensçš„é‡å ï¼‰
        chunk_size = (total_tokens // num_chunks) + 200
        
        return min(chunk_size, safe_limit)

def main():
    # æ·»åŠ æ¨¡å‹é€‰æ‹©
    print("\né€‰æ‹©å¤§æ¨¡å‹:")
    print("1. OpenAI")
    print("2. DeepSeek")
    model_choice = input("è¯·è¾“å…¥é€‰é¡¹ç¼–å·: ")

    model_type = "openai" if model_choice == "1" else "deepseek"

    # ç®€åŒ–åˆå§‹åŒ–ï¼Œç§»é™¤ provider_type å‚æ•°
    analyzer = ArxivPaperAnalyzer(model_type=model_type)
    
    while True:
        print("\n=== arXivè®ºæ–‡åˆ†æå·¥å…· ===")
        print("1. æŒ‰é¢†åŸŸæµè§ˆæœ€æ–°è®ºæ–‡")
        print("2. æŒ‰å…³é”®è¯æœç´¢è®ºæ–‡")
        print("3. æŒ‰å…³é”®è¯åœ¨æŒ‡å®šé¢†åŸŸæœç´¢")
        print("4. é€€å‡º")
        
        choice = input("\nè¯·è¾“å…¥é€‰é¡¹ç¼–å·: ")
        
        try:
            paper_count = 0
            if choice in ['1', '2', '3']:
                paper_count = int(input("è¯·è¾“å…¥è¦åˆ†æçš„è®ºæ–‡æ•°é‡ (1-50): "))
                if not (1 <= paper_count <= 50):
                    print("è¯·è¾“å…¥1åˆ°50ä¹‹é—´çš„æ•°å­—ã€‚")
                    continue
                
                analysis_choice = input("è¯·é€‰æ‹©åˆ†ææ–¹å¼: 1) ä»…åˆ†ææ ‡é¢˜å’Œæ‘˜è¦ 2) åˆ†æå…¨æ–‡\nè¯·è¾“å…¥1æˆ–2: ")
                analyze_full_text = analysis_choice == '2'
                
                # æ·»åŠ æ—¥æœŸèŒƒå›´é€‰æ‹©
                use_date_range = input("æ˜¯å¦éœ€è¦æŒ‡å®šæ—¶é—´èŒƒå›´ï¼Ÿ(y/n): ").lower() == 'y'
                date_start = date_end = None
                if use_date_range:
                    date_start = input("è¯·è¾“å…¥å¼€å§‹æ—¥æœŸ (YYYY-MM-DD): ")
                    date_end = input("è¯·è¾“å…¥ç»“æŸæ—¥æœŸ (YYYY-MM-DD): ")
            
            if choice == "1":
                analyzer.analyze_recent_papers(
                    max_papers=paper_count,
                    analyze_full_text=analyze_full_text
                )
                
            elif choice == "2":
                query = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯: ")
                analyzer.search_and_analyze_papers(
                    query=query,
                    max_papers=paper_count,
                    analyze_full_text=analyze_full_text,
                    date_start=date_start,
                    date_end=date_end
                )
                
            elif choice == "3":
                query = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯: ")
                print("\nè¯·é€‰æ‹©æœç´¢é¢†åŸŸ:")
                field = analyzer.category_matcher.interactive_select()
                analyzer.search_and_analyze_papers(
                    query=query,
                    max_papers=paper_count,
                    analyze_full_text=analyze_full_text,
                    field=field,
                    date_start=date_start,
                    date_end=date_end
                )
                
            elif choice == "4":
                print("æ„Ÿè°¢ä½¿ç”¨ï¼Œå†è§ï¼")
                break
                
            else:
                print("æ— æ•ˆçš„é€‰é¡¹ï¼Œè¯·é‡æ–°é€‰æ‹©ã€‚")
                
        except ValueError:
            print("è¯·è¾“å…¥æœ‰æ•ˆçš„æ•°å­—ã€‚")
        except Exception as e:
            print(f"æ“ä½œè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            continue

if __name__ == "__main__":
    main() 